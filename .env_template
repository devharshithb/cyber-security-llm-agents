LLM_WORKING_FOLDER = "llm_working_folder"

# LLM Configuration
# Backend options: "ollama" (local, free), "openai" (requires API key), "groq" (free tier with API key)
LLM_BACKEND = "ollama"

# Ollama settings (default - no API key needed)
OLLAMA_BASE_URL = "http://localhost:11434"
OLLAMA_MODEL = "llama2"

# OpenAI settings (optional - only if using openai backend)
OPENAI_MODEL_NAME = "gpt-3.5-turbo"
OPENAI_API_KEY = ""

# Groq settings (optional - free tier available at https://console.groq.com)
GROQ_API_KEY = ""
GROQ_MODEL = "llama3-8b-8192"

# Caldera integration (optional)
CALDERA_SERVER = "http://<caldera hostname>:8888"
CALDERA_API_KEY = "<CALDERA API KEY>"

# Optional - Only required if you plan to run the integrated Web and/or FTP server to 
# serve files and to exfiltrate data using agents. Used for our talk at RSAC24.
WEB_SERVER_PORT = 8800
FTP_SERVER_ADDRESS = "192.168.162.11:2100"
FTP_SERVER_USER = "user"
FTP_SERVER_PASS = "12345"
